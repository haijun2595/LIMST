#R
#Match collation data
base_path <- "//path//"
expr_matrices <- list()
clinical_datas <- list()

folders <- c("GBM", "Melanoma_GSE91061","Melanoma_phs000452","nonsqNSCLC_GSE93157",
             "NSCLC_GSE135222","RCC_Braun_2020","STAD-PRJEB25780")

for (folder in folders) {
  expr_matrix_file <- paste0(base_path, folder, "//", folder, ".Response.Rds")
  expr_matrix <- readRDS(expr_matrix_file)
  expr_matrix <- as.data.frame(expr_matrix)
  rownames(expr_matrix) <- expr_matrix$GENE_SYMBOL
  expr_matrix$GENE_SYMBOL <- NULL
  expr_matrix <- as.matrix(expr_matrix)
  
  clinical_data_file <- paste0(base_path, folder, "//", folder, ".Response.TSV")
  clinical_data <- read.table(clinical_data_file, header=TRUE, sep="\t", stringsAsFactors=FALSE)
  clinical_data = clinical_data[,c('sample_id', 'response')]
  clinical_data <- as.data.frame(clinical_data)
  rownames(clinical_data) <- clinical_data$sample_id
  clinical_data$sample_id <- NULL
  
  expr_matrices[[folder]] <- expr_matrix
  clinical_datas[[folder]] <- clinical_data
}

# 将 expr_matrices 和 clinical_datas 列表合并成单个矩阵和数据框
expr_matrix <- do.call(cbind, expr_matrices)
clinical_data <- do.call(rbind, clinical_datas)
expr_matrix <- expr_matrix[, !colnames(expr_matrix) %in% c(
  "SRR5088827", "SRR5088828", "SRR5088860", "SRR5088863", "SRR5088865", 
  "SRR5088868", "SRR5088869", "SRR5088873", "SRR5088874", "SRR5088875", 
  "SRR5088876", "SRR5088881", "SRR5088884", "SRR5088901", "SRR5088902", 
  "SRR5088930","SRR5088919","SRR5088817","SRR5088818","SRR5088844"
)]
rownames(clinical_data) <- colnames(expr_matrix)

colnames(expr_matrix)
rownames(clinical_data)
all(rownames(clinical_data) %in% colnames(expr_matrix))

if(all(rownames(clinical_data) %in% colnames(expr_matrix))) {
  new_clinical_data <- merge(clinical_data, t(expr_matrix), by = "row.names", all.x = TRUE)
  new_clinical_data <- new_clinical_data[!is.na(new_clinical_data$response), ] 
}

new_clinical_data <- new_clinical_data[new_clinical_data$response != "UNK", ]

write.csv(new_clinical_data,"//path//data.csv")

RNA <- read.csv("//path//data.csv")
DEG <- read.csv("//path//DEG.csv")

gene_names <- DEG[["gene"]]

all_columns_exist <- all(gene_names %in% colnames(RNA))

if (!all_columns_exist) {
  missing_columns <- gene_names[!gene_names %in% colnames(RNA)]
  print(paste("以下列名在 RNA 中不存在:", paste(missing_columns, collapse=", ")))
} else {
  print("gene_names 中的所有列名在 RNA 中都存在")
}

gene_names <- gene_names[!gene_names %in% missing_columns]

RNA_matched <- RNA[,gene_names ]

write.csv(RNA_matched,"//path//matched.csv")

#python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, auc
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# Load data
data = pd.read_csv('matched.csv')

data['response'] = data['response'].replace({'N': 0, 'R': 1})

print(data)

# Calculate missing values 
missing_values_columns = data.isnull().sum()
print("Missing values per column:\n", missing_values_columns)

missing_values_rows = data.isnull().sum(axis=1)
print("\nMissing values per row:\n", missing_values_rows)


# Remove  missing values
data_no_missing_rows = data.dropna()
data_no_missing_columns = data.dropna(axis=1)

y = data["response"]  
X = data.drop(["response"], axis=1)  

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1664676136)

# Model and parameter settings
models = {
    'Random Forest': {
        'model': RandomForestClassifier(random_state=1664676136),
        'params': {
            'n_estimators': [90, 420, 930],
            'max_depth': [5, 500, 5000],
            'min_samples_split': [2, 4, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': [None, 'sqrt', 'log2']
        }
    },
    'Gradient Boosting': {
        'model': GradientBoostingClassifier(random_state=1664676136),
        'params': {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.01, 0.1, 0.3],
            'max_depth': [3, 5, 10],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': [None, 'sqrt', 'log2']
        }
    },
    'SVM': {
        'model': SVC(probability=True, random_state=1664676136),
        'params': {
            'C': [0.1, 1, 10],
            'kernel': ['linear', 'rbf', 'poly'],
            'gamma': ['scale', 'auto']
        }
    },
    'KNN': {
        'model': KNeighborsClassifier(),
        'params': {
            'n_neighbors': [10, 100, 200],
            'weights': ['uniform', 'distance'],
            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
            'leaf_size': [20, 30, 40],
            'p': [1, 2]
        }
    },
    'Naive Bayes': {
        'model': GaussianNB(),
        'params': {
            'var_smoothing': [1e-9, 1e-8, 1e-7]
        }
    },
    'Logistic Regression': {
        'model': LogisticRegression(random_state=1664676136),
        'params': {
            'C': [0.1, 1, 10],
            'penalty': ['l1', 'l2'],
            'solver': ['liblinear', 'saga']
        }
    },
    'MLP': {
        'model': MLPClassifier(random_state=1664676136),
        'params': {
            'hidden_layer_sizes': [(100,), (200,), (50, 100)],
            'activation': ['relu', 'tanh'],
            'alpha': [0.0001, 0.001, 0.1],
            'solver': ['lbfgs', 'sgd', 'adam'],
            'learning_rate': ['constant', 'invscaling', 'adaptive'],
            'learning_rate_init': [0.001, 0.01, 0.1],
            'max_iter': [200, 400, 800],
            'early_stopping': [True, False]
        }
    },
    'Decision Tree': {
        'model': DecisionTreeClassifier(random_state=1664676136),
        'params': {
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['auto', 'sqrt', 'log2']
        }
    },
    'AdaBoost': {
        'model': AdaBoostClassifier(random_state=1664676136),
        'params': {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.01, 0.1, 0.5]
        }
    },
    'XGBoost': {
        'model': XGBClassifier(objective='binary:logistic', random_state=1664676136),
        'params': {
            'n_estimators': [100, 500, 1000],
            'learning_rate': [0.01, 0.1, 0.3],
            'max_depth': [3, 5, 10],
            'min_child_weight': [1, 3, 5]
        }
    },
    'LightGBM': {
        'model': LGBMClassifier(random_state=1664676136),
        'params': {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.01, 0.1, 0.5],
            'max_depth': [3, 5, 10],
            'reg_alpha': [0.0, 0.1, 1, 10, 100],
            'reg_lambda': [0.0, 0.1, 1, 10, 100]
        }
    },
    'CatBoost': {
        'model': CatBoostClassifier(random_state=1664676136, verbose=0),
        'params': {
            'iterations': [1000, 5000, 10000, 20000],
            'learning_rate': [0.01, 0.1, 0.3],
            'depth': [4, 6, 10, 11],
            'l2_leaf_reg': [3, 7],
            'grow_policy': ['SymmetricTree']
        }
    }
}

# Function for model training and evaluation
def train_and_evaluate(model, params, X_train, y_train, X_test, y_test):
    random_search = RandomizedSearchCV(model, params, cv=5, scoring='roc_auc', n_jobs=-1, random_state=1664676136)
    random_search.fit(X_train, y_train)
    best_model = random_search.best_estimator_
    
    # Training set prediction
    train_pred_probs = best_model.predict_proba(X_train)[:, 1]
    train_auc = roc_auc_score(y_train, train_pred_probs)
    train_acc = accuracy_score(y_train, best_model.predict(X_train))
    
    # Test set prediction
    test_pred_probs = best_model.predict_proba(X_test)[:, 1]
    test_auc = roc_auc_score(y_test, test_pred_probs)
    test_acc = accuracy_score(y_test, best_model.predict(X_test))
    
    return best_model, train_pred_probs, test_pred_probs, train_auc, test_auc, train_acc, test_acc

# Train and evaluate all models
results = {}
train_pred_probs_all = {}
test_pred_probs_all = {}
for name, model_info in models.items():
    best_model, train_pred_probs, test_pred_probs, train_auc, test_auc, train_acc, test_acc = train_and_evaluate(
        model_info['model'], model_info['params'], X_train, y_train, X_test, y_test
    )
    results[name] = {
        'Train AUC': train_auc,
        'Test AUC': test_auc,
        'Train Accuracy': train_acc,
        'Test Accuracy': test_acc
    }
    train_pred_probs_all[name] = train_pred_probs
    test_pred_probs_all[name] = test_pred_probs

# Print results
for name, metrics in results.items():
    print(f"{name} Classifier:")
    print(f"Train AUC: {metrics['Train AUC']:.2f}")
    print(f"Test AUC: {metrics['Test AUC']:.2f}")
    print(f"Train Accuracy: {metrics['Train Accuracy']:.2f}")
    print(f"Test Accuracy: {metrics['Test Accuracy']:.2f}\n")

# Plot ROC curve
def plot_roc_curve(y_true, y_pred_probs, label=None):
    fpr, tpr, _ = roc_curve(y_true, y_pred_probs)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'{label} (AUC = {roc_auc:.2f})')

plt.figure(figsize=(14, 10))

# Plot ROC curve for the training set
plt.subplot(2, 1, 1)
for name, pred_probs in train_pred_probs_all.items():
    plot_roc_curve(y_train, pred_probs, label=name)
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Various Classifiers (Train Set)')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)

#Plot ROC curve for the test set
plt.subplot(2, 1, 2)
for name, pred_probs in test_pred_probs_all.items():
    plot_roc_curve(y_test, pred_probs, label=name)
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Various Classifiers (Test Set)')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

#Importance of features
cat_classifier = CatBoostClassifier(random_state=1664676136, verbose=0, thread_count=-1)
cat_classifier.fit(X_train, y_train)

feature_importance = cat_classifier.feature_importances_

feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})

feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

print(feature_importance_df)

plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('CatBoost Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.savefig('/path/catboost_feature_importance.png')

# SHAP
explainer = shap.TreeExplainer(best_catboost_classifier)
shap_values = explainer.shap_values(X_train)

shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[0], X_train.iloc[0])

shap.summary_plot(shap_values, X_train, feature_names=X_train.columns)